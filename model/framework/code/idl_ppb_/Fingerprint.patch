--- /home/cflou/Project/AttentiveFP/T_attentive_cuda/AttentiveFP/AttentiveLayers.py
+++ /home/cflou/Project/AttentiveFP/T_attentive_cuda/AttentiveFP/AttentiveLayers.py
@@ -42,14 +42,12 @@
         attend_mask = atom_degree_list.clone()
         attend_mask[attend_mask != mol_length-1] = 1
         attend_mask[attend_mask == mol_length-1] = 0
-        attend_mask = attend_mask.type(torch.FloatTensor).unsqueeze(-1) #####changed to CPU
-        #attend_mask = attend_mask.type(torch.cuda.FloatTensor).unsqueeze(-1)
+        attend_mask = attend_mask.type(torch.cuda.FloatTensor).unsqueeze(-1)
 
         softmax_mask = atom_degree_list.clone()
         softmax_mask[softmax_mask != mol_length-1] = 0
         softmax_mask[softmax_mask == mol_length-1] = -9e8 # make the softmax value extremly small
-        softmax_mask = softmax_mask.type(torch.FloatTensor).unsqueeze(-1) #####changed to CPU
-        #softmax_mask = softmax_mask.type(torch.cuda.FloatTensor).unsqueeze(-1)
+        softmax_mask = softmax_mask.type(torch.cuda.FloatTensor).unsqueeze(-1)
 
         batch_size, mol_length, max_neighbor_num, fingerprint_dim = neighbor_feature.shape
         atom_feature_expand = atom_feature.unsqueeze(-2).expand(batch_size, mol_length, max_neighbor_num, fingerprint_dim)
@@ -113,8 +111,7 @@
         mol_softmax_mask = atom_mask.clone()
         mol_softmax_mask[mol_softmax_mask == 0] = -9e8
         mol_softmax_mask[mol_softmax_mask == 1] = 0
-        #mol_softmax_mask = mol_softmax_mask.type(torch.cuda.FloatTensor)
-        mol_softmax_mask = mol_softmax_mask.type(torch.FloatTensor)
+        mol_softmax_mask = mol_softmax_mask.type(torch.cuda.FloatTensor)
         
         for t in range(self.T):
             